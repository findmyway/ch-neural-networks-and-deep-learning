人的视觉感官是这世上最神奇的事情之一。看看下面这串手写字：

![](http://neuralnetworksanddeeplearning.com/images/digits.png)

大多数人都可以毫不费力地识别出这串数字是504192。看起来似乎是件很容易的事，事实上，在人类的每个脑半球中都有一个初级视觉皮层，也被称作V1层，包含了约1.4亿个神经元，这之间又有数百亿的连接。除此之外还有一系列的视觉皮层，如V2、V3、V4和V5层，负责处理更复杂的图像处理。我们可以把大脑看做是一台超级计算机，在经历了上亿年的进化后，才适应了我们所看到的世界。识别手写数字并不容易。尽管人类非常擅长理解双眼所看到的这世界，但由于识别数字几乎是在潜意识中就完成了，以至于我们并不惊讶人类的视觉神经系统解决了这么复杂的问题。

当你动手去写个程序来解决上面的手写字识别时，你就会明白这个问题有多难了。在我们看来很简单的问题似乎一下子变得相当困难了。想想我们是怎么描述一个数字的，比如“9”，“就是一个圈圈带着一个尾巴~”，不过这个描述很难从算法层面上来描述。当你试图使用更加精细的规则来加以描述时，你会陷入到种种例外、特殊情况等等复杂的细节中。唉，似乎很绝望......

神经网络通过另外一种办法来解决这个问题。其核心思想是使用下图所示大量的手写数字作为训练样本。

![](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)

然后，训练系统从上面的样本中去学习。也就是说，神经网络使用样本来自动学习识别手写字说使用的规则。也就是说，通过增加训练样本，神经网络能够从手写字中学到更多，从而提高准确率。尽管上面只列出了100幅图，不过我们可以通过成千上万的样本学习。

在本章，我们会写一个程序来构建神经网络识别这些手写字。整个程序只有72行，而且并没有采用其他的神经网络工具包。但使用这个程序就能够是我们识别数字的准确率达到96%以上。在接下来的章节中，我们会继续深入，将准确率提高到99%以上。事实上，最好的商业神经网络系统已被用在银行支票和邮局的手写字识别系统中。

本书专注于手写字的识别，这是因为在神经网络系统的学习中，手写字的识别是个比较基本的问题，其主要好处在于，这个问题有一定的挑战性――既不太简单，但也不需要多么高深的解决方案或是强大的计算能力。而且通过这个问题还可以学到一些其它的高级技巧，比如深度学习。因此呢，手写数字的识别将贯穿全书。在本书接下来的内容中，我们会讨论如何将这些思想运用到计算机视觉以及语音和自然语言处理等其它领域中。

当然，如果本章仅仅是写一个程序来解决手写数字识别的问题，那么这一章将会短得多。但在这里我们会介绍许多神经网络中的核心基础概念包括两种类型的人工神经元（感知器和sigmoid神经元），以及标准的神经网络算法如SGD（stochastic gradient descent）本章主要是讲清楚为什么它们是这个样子的，再此基础之上，构建对于神经网络的认识。这比简单地告诉你神经网络工作原理要稍微显得嗦一些，不过这会让你对神经网络有更深入的认识，我认为这是值得的。最后我们来理解深度学习究竟是什么以及为什么它如此重要。

##感知器

什么是神经网络呢？首先，来介绍一类人工神经元，感知器。感知器最早由[Frank Rosenblatt](http://en.wikipedia.org/wiki/Frank_Rosenblatt) 在 [Warren McCulloch](http://en.wikipedia.org/wiki/Warren_McCulloch)和[Walter Pitts](http://en.wikipedia.org/wiki/Walter_Pitts)的[研究](http://scholar.google.ca/scholar?cluster=4035975255085082870)基础上[提出](http://books.google.ca/books/about/Principles_of_neurodynamics.html?id=7FhRAAAAMAAJ)来的。在如今的一些神经网络中，一类称为*sigmoid 神经元*的人工神经元用得更多，本书和一些现代的神经网络中也主要使用这类神经元。不过要想理解为什么sigmoid神经元是这么定义的，我们需要先深入了解下感知器的原理。

那，感知器是怎么工作的呢？一个感知器接收一些二值化的输入$x_1,x_2,\ldots$,然后产生一个二值化的输出：

![](http://neuralnetworksanddeeplearning.com/images/tikz0.png)

上面这个例子中，感知器有三个输入，$x_1,x_2,x_3$。当然，输入的个数可以更多或者更少。Rosenblatt提出了一个简单的规则来计算输出。他引入了*权重*，$w_1,w_2,\ldots$,来描述输入对于输出的影响程度。感知器的结果要么是0要么是1，其值是由输入层的加权和$\sum_j w_j x_j$ 是否大于某一阈值来决定的。和权值一样，阈值也是感知器的一个参数。用数学的表达方式来描述就是下面这个式子。
<p>
\begin{eqnarray}
  \mbox{output} & = & \left\{ \begin{array}{ll}
      0 & \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\
      1 & \mbox{if } \sum_j w_j x_j > \mbox{ threshold}
      \end{array} \right.
\tag{1}\end{eqnarray}
</p>

这是最基本的数学模型。你可以这么理解，感知器就是把你所知道的信息通过加权后用来做决策。举个简单的例子，比如，周末马上就要到了，市区会有个奶酪节，你很喜欢奶酪，不知道去还是不去。你需要综合考虑下面三个因素：

1. 天气好不好？

2. 你男朋友或者女朋友陪不陪你一起去？

3. 举办奶酪节的地方交通方不方便？

我们可以用变量$x_1，x_2$和$x_3$来表示这三个因素。例如，如果天气好的话那么$x_1=1$，否则的话$x_1=0$，类似的$x_2=1$表示你男朋友或者女朋友想一起去，否则的话$x_2=0$,$x_3$也类似。

现在假设你非常喜欢奶酪，所以呢，木有女朋友陪也没关系，哈哈。但是你很在意天气，如果天气很差的话你是不会去的。你可以用感知器来描述这个决策。一种方法是，给天气赋权值$w_1=6$，另外两个变量$w_2=2$，$w_3=2$分别代表另外两个条件。天气的权值较大，意味着天气的因素对你来说更重要，比女朋友的因素更重要哦~最后，我们设定这个感知器的阈值是5。也就意味着，天气好的话感知器的输出是1，天气不好的话，感知器的输出是0，至于女朋友去不去，交通方不方便这些都不重要了。

通过改变权值和阈值，我们能得到不同的模型。例如，假设阈值是3，这就意味着，a.天气好 b.女朋友一起去而且交通方便 这两个条件中任何一个成立时，你都会去。也就是说，这变成了另外一个不同的模型，降低阈值意味着你更想去参加这个奶酪节。

很明显，感知器并不是描述人类做决策的完整模型。但上面的例子描述了感知器是如何对不同的因素加权后用于最终决策的过程。我们有理由相信一个完整的感知器网络可以用来做决策。

![](http://neuralnetworksanddeeplearning.com/images/tikz1.png)

上面这个神经网络中的第一层，对输入作加权求和。那么第二层呢？第二层的感知器对第一层感知器的输出做加权求和。也就是说，第二层感知器相比第一层的感知器而言，要在更加复杂而抽象的层面做出决策。第三层的感知器做更复杂的决策。这就意味着，多层的神经网络可以模拟一个相当复杂的决策。

一开始定义感知器的时候，我们说感知器只有一个输出，但是上面的神经网络看起来似乎有不止一个输出。事实上，这些感知器仍然只有一个输出，只不过，上面那些感知器输出上的箭头是为了表示该输出又作用到下一层的输入上了，这样子画看起来简洁一些，不然我们得先画一个输出然后该输出又分裂并连接到下一层的输入。

简单描述一下感知器。前面的描述$\sum_j w_j x_j > threshold$ 显得有些累赘。我们可以进一步将其简化。首先，将$\sum_j w_j x_j$ 改写为向量的点乘形式。$w \cdot x \equiv \sum_j w_j x_j$,其中$w$和$x$分别表示权值和输入的向量。另一个改变是将threshold移到不等式的另一边，同时用另外一个叫做*偏差*的变量来代替，其中$b \equiv - threshold$。最后感知器模型被改写为下面的式子：

<p>
\begin{eqnarray}
  \mbox{output} = \left\{ 
    \begin{array}{ll} 
      0 & \mbox{if } w\cdot x + b \leq 0 \\
      1 & \mbox{if } w\cdot x + b > 0
    \end{array}
  \right.
\tag{2}\end{eqnarray}
</p>

你可以把偏差看做是感知器偏向于输出1的程度，换个说法，就是激活感知器的程度。对于一个有较大偏差的感知器来说，更容易输出1，而对于偏差值很小（一个很小的负数）的感知器来说，很难输出1.添加对**偏差**的描述只是我们对感知器的改进之一，接下来会看到更多的简化。在后面的内容中，本书不再使用**阈值**，而是**偏差**。

我们提到了感知器可以用来做决策，除此之外，还可以用来做一些逻辑运算，如 与、或、非等操作。例如，假设一个感知器有两个输入，每个的权重是-2，同时偏差是3.如下图所示：

![](http://neuralnetworksanddeeplearning.com/images/tikz2.png)

可以看到输入$0,0$ 的结果是 $1$, 因为$(-2)*0+(-2)*0+3=3$是大于0的正数。类似的运算可以得到$输入 0 1 和 1 0 都会得到结果 1$, 但是输入1 1 会得到结果0，因为$(-2)*1+(-2)*1+3=-1$是个负数。这样我们便得到了一个**与非门**。

上面的这个例子表明，我们可以用感知器来计算简单的逻辑关系。事实上，我们可以感知器网络来描述任意的逻辑关系。这是因为，呵呵，学过离散数学你就懂了。通过与非门可以构造出任意的逻辑关系。例如，构造一个计算按位求和的电路。$x_1 \oplus x_2$，当$x_1 和x_2都是1的时候，进位标识（carry）为1$

![](http://neuralnetworksanddeeplearning.com/images/tikz3.png)

我们用感知器网络来表述上面的电路图如下。

![](http://neuralnetworksanddeeplearning.com/images/tikz4.png)

上面这个图中，值得注意的一点是，最左边那个感知器的输出传递了两倍给给下面那个感知器的输入。在前面定义感知器模型的时候，并没有提到是否可以这样子做。事实上，这对整个模型并没有什么影响。如果模型本身并不允许输入多个来自同一感知器输出的话，那么我们可以简单地将这两条线融合成一条线，然后调整权值为原来的两倍就行了。这样，就变成了下图：（注意权重的变化）

![](http://neuralnetworksanddeeplearning.com/images/tikz5.png)

到目前为止，我们一直将$x_1 和 x_2$ 作为一个变量放在网络的最左边，事实上，我们可以将其作为网络的输入层，如下图表示：

![](http://neuralnetworksanddeeplearning.com/images/tikz6.png)

对于输入层的感知器来说，它只有输出没有输入。

![](http://neuralnetworksanddeeplearning.com/images/tikz7.png)

其实，输入层的感知器不过是一个普通的感知器。我们可以这么来看。既然输入为零，那么也就是说$\sum_j w_j x_j$的和为0，然后如果$b>0$的话，输出为1，如果$b \leq 0$，输出为0。也就是说，输入层的感知器只是简单的输出一个固定值，而不受变量($x_1 ...$)等的影响。简单理解为输入我们想要的值，如$x_1,x-2,x_3 ...$

加法器的例子展示了神经网络是怎样用于模拟与非门的，由于一般的运算都可以通过与非门来完成，因此我们可以理解为感知器可以完成任意的计算。感知器的通用性这点既让人兴奋又让人感到有些沮丧，说兴奋是因为感知器可以和其他设备一样强大完成任意计算，说沮丧是因为，感知器不过是又一种形式的与非门罢了。这可不是什么新鲜事。

不过呢，情况要比表面上看上去的要好点。我们可以设计出一种学习算法让感知器组成的网络自动去学习权重和偏差的大小。所有的过程由外部的输入来主导，程序员并不涉及内部的具体过程。这种学习算法使得我们能够以一种不同于对待逻辑运算器的方式来使用感知器。从而轻松解决传统中由逻辑与或非组成的集成电路所不能解决的问题。

##sigmoid神经元
学习算法听起来很牛但是我们怎么利用神经网络来实现呢？假设我们已经有一个由感知器组成的网络，例如，输入可能是一些扫描的数字图像中的像素值。我们希望该网络能够自动学习权重和偏差从而利用其输出来对数字分类，以达到识别的目的。首先来了解下学习的过程。假设我们对神经网络中的权值或者偏差施加一个小的扰动，我们希望这个扰动对神经网络的输出也会产生一个小的扰动，待会就能看到这个特性让自动学习的过程变成可能。下图是一个简单的说明：

![](http://neuralnetworksanddeeplearning.com/images/tikz8.png)

现在假设权值上很小的改变对于输出结果也只有小的改变，那么我们就能利用这个特性得到我们想要的结果。具体来说是这样子的，假设该神经网络错误地将数字9识别成了数字8，通过改变神经网络中某些权值，我们能够微弱地改变输入使得其结果更倾向于将数字识别成9.重复这个过程，并反复改变权值，那么我们最终就能得到想要的结果。同时意味着该神经网络完成了自我学习的过程。

但现在的问题是，如果采用感知器构成的神经网络，并没有前面提到的那种特性。事实上，网络中对权值或者偏差的任何微弱扰动，最终会导致感知器的输出发生很大的突变，比如从0跳变到1。这会使得整个网络的输出变得非常复杂而且不稳定。比如，通过对权值做一个微小的调整，这幅图可能准确地识别成为了9，但是这组权值对于剩下的所有图像的识别结果可能全都变了。这种突变很难让我们以一种渐变的方式去修改权值以得到想要的结果。也许有那么一种方法可以解决这个问题，但目前为止还没有一种很有效的方法来让神经网络在这样的情况下学习。

不过，我们可以通过引入一种新的神经元――“sigmoid神经元”来解决上面的问题。“Sigmoid神经元”和感知器很像，不过呢，权值和偏差的微弱改变对于其输出也只会产生微弱的改变。这就使得由sigmoid神经元构成的网络恩呢该个自适应地去学习权值。

首先来描述一下sigmoid神经元：

![](http://neuralnetworksanddeeplearning.com/images/tikz9.png)

和感知器一样，sigmoid神经元也有输入$x_1,x_2,\ldots$但是，它的输出不再是0或者1 。而是输出0到1之间的某个值。例如0.638等等。类似的，sigmoid神经元也有权值$w_1,w_2,\ldots$以及一个全局偏差b。但是输出不再是0或者1，而是$\sigma(w \cdot x+b)$ 其中$\sigma$叫做**sigmoid函数**，其定义如下：

> 有时候，$\sigma$也称为**逻辑函数，这种类型的神经元也被称为逻辑神经元。记住这个术语有时候很有用，那些学术背景为神经网络方面的人往往会用这个概念。不过下面我还是会使用sigmoid神经元这个术语。

<p>
\begin{eqnarray} 
  \sigma(z) \equiv \frac{1}{1+e^{-z}}.
\tag{3}\end{eqnarray}
</p>

为了将上面的式子表述得更清楚点，我们用输入$x_1,x_2,\ldots$和权重$w_1,w_2,\ldots以及偏差b$来表述：

<p>
\begin{eqnarray} 
  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}.
\tag{4}\end{eqnarray}
</p>

乍看上去，sigmoid神经元和感知器很不一样。如果你对sigmoid函数不熟的话，上面这个式子可能让人感觉高深莫测。实际上，sigmoid神经元和前面的感知器有很多相似之处。而且上面那个虽然看起来很高深，其实只是一些数学上的处理技巧罢了，理解起来并不困难。

先来说明下和感知器模型的相似之处。假设$z \equiv w \cdot x + b$是一个很大的正整数，那么$e^{-z} \approx 0$ 从而$\sigma(z) \approx 1$。也就是说，当$z=w \cdot x + b$是一个很大的正数时，sigmoid神经元的输出接近于1.也就和感知器是一样的了。类似的，假设$z = w \cdot x +b $是一个很大的负数，那么$\sigma^{-z} \rightarrow \infty$ 从而$\sigma(z) \approx 0$。也即是说，当$z=w \cdot x + b $是一个很的负数时，sigmoid神经元的输出也非常接近感知器的输出。只有当$w \cdot x + b $的值位于0附近的时候，它的输出才和感知器有很大差别。

那么怎么理解sigmoid函数呢？事实上$\sigma$函数的具体形式并不重要，重要的是这个函数的形状，如下图所示：

<script src="http://d3js.org/d3.v2.min.js"></script>

<div id="sigmoid_graph"></div>

<script>
function s(x) {return 1/(1+Math.exp(-x));}
var m = [40, 120, 50, 120];
var height = 290 - m[0] - m[2];
var width = 600 - m[1] - m[3];
var xmin = -5;
var xmax = 5;
var sample = 400;
var x1 = d3.scale.linear().domain([0, sample]).range([xmin, xmax]);
var data = d3.range(sample).map(function(d){ return {
        x: x1(d), 
        y: s(x1(d))}; 
    });
var x = d3.scale.linear().domain([xmin, xmax]).range([0, width]);
var y = d3.scale.linear()
                .domain([0, 1])
                .range([height, 0]);
var line = d3.svg.line()
    .x(function(d) { return x(d.x); })
    .y(function(d) { return y(d.y); })
var graph = d3.select("#sigmoid_graph")
    .append("svg")
    .attr("width", width + m[1] + m[3])
    .attr("height", height + m[0] + m[2])
    .append("g")
    .attr("transform", "translate(" + m[3] + "," + m[0] + ")");
var xAxis = d3.svg.axis()
                  .scale(x)
                  .tickValues(d3.range(-4, 5, 1))
                  .orient("bottom")
graph.append("g")
    .attr("class", "x axis")
    .attr("transform", "translate(0, " + height + ")")
    .call(xAxis);
var yAxis = d3.svg.axis()
                  .scale(y)
                  .tickValues(d3.range(0, 1.01, 0.2))
                  .orient("left")
                  .ticks(5)
graph.append("g")
    .attr("class", "y axis")
    .call(yAxis);
graph.append("path").attr("d", line(data));
graph.append("text")
     .attr("class", "x label")
     .attr("text-anchor", "end")
     .attr("x", width/2)
     .attr("y", height+35)
     .text("z");
graph.append("text")
        .attr("x", (width / 2))             
        .attr("y", -10)
        .attr("text-anchor", "middle")  
        .style("font-size", "16px") 
        .text("sigmoid function");
</script>

上面这个形状实际上是0-1函数的一个平滑后的效果：

<div id="step_graph"></div>

<script>
function s(x) {return x < 0 ? 0 : 1;}
var m = [40, 120, 50, 120];
var height = 290 - m[0] - m[2];
var width = 600 - m[1] - m[3];
var xmin = -5;
var xmax = 5;
var sample = 400;
var x1 = d3.scale.linear().domain([0, sample]).range([xmin, xmax]);
var data = d3.range(sample).map(function(d){ return {
        x: x1(d), 
        y: s(x1(d))}; 
    });
var x = d3.scale.linear().domain([xmin, xmax]).range([0, width]);
var y = d3.scale.linear()
                .domain([0,1])
                .range([height, 0]);
var line = d3.svg.line()
    .x(function(d) { return x(d.x); })
    .y(function(d) { return y(d.y); })
var graph = d3.select("#step_graph")
    .append("svg")
    .attr("width", width + m[1] + m[3])
    .attr("height", height + m[0] + m[2])
    .append("g")
    .attr("transform", "translate(" + m[3] + "," + m[0] + ")");
var xAxis = d3.svg.axis()
                  .scale(x)
                  .tickValues(d3.range(-4, 5, 1))
                  .orient("bottom")
graph.append("g")
    .attr("class", "x axis")
    .attr("transform", "translate(0, " + height + ")")
    .call(xAxis);
var yAxis = d3.svg.axis()
                  .scale(y)
                  .tickValues(d3.range(0, 1.01, 0.2))
                  .orient("left")
                  .ticks(5)
graph.append("g")
    .attr("class", "y axis")
    .call(yAxis);
graph.append("path").attr("d", line(data));
graph.append("text")
     .attr("class", "x label")
     .attr("text-anchor", "end")
     .attr("x", width/2)
     .attr("y", height+35)
     .text("z");
graph.append("text")
        .attr("x", (width / 2))             
        .attr("y", -10)
        .attr("text-anchor", "middle")  
        .style("font-size", "16px") 
        .text("step function");
</script>

如果$\sigma$是一个0-1函数的话，那么sigmoid神经元实际上就是一个感知器，因为输出就是0或1，由输入$w \cdot x + b$的正负决定。实际上，最关键的地方是$\sigma$函数的平滑程度而不是它的具体形式。$\sigma$的平滑程度以为着$\Delta w_j\$和$\Delta b$的微小变化会对$\Delta output $产生一个小的改变。可以将$\Delta output$近似地表示成下式：

<p>
\begin{eqnarray} 
  \Delta \mbox{output} \approx \sum_j \frac{\partial \, \mbox{output}}{\partial w_j}
  \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b} \Delta b,
\tag{5}\end{eqnarray}
</p>

其中，求和符号是指对所有的权重$w_j$求和，而$\partial output / \partial w_j 和 \partial output /  \partial b$分别表示output对$w_j和b$的偏导。啊哦，不知道什么是偏导？（赶紧复习下微积分去）尽管上面的表达是看起来有点复杂，但是，它所表达的意思很简单：$\Delta output 是 \Delta w_j 和 \Delta b$的线性函数。这种线性关系使得我们可以通过对权重和偏差做一些小的改变从而得到我们想要的输出的改变。因此相比感知器而言，sigmoid神经元更容易看出改变权重和偏差是如何影响输出的。

那么既然重要的是$\sigma$函数的形状而不是其具体的表达式，那么为什么恰恰使用等式3呢？实际上在后面我们还会使用另外一种形式的激活函数$f(w \cdot x + b)$，最主要的一个变化是，等式5的偏导不同了，对比$\sigma$函数，我们后面将要使用的那种函数的偏导形式要比等式5所表示的$\sigma$函数偏导形式复杂很多。事实上，在神经网络中，sigmoid函数是最常用的一种神经元，因此在本书的后面我们也主要采用这种形式的激活函数。

那么怎么解释sigmoid神经元的输出呢？sigmoid神经元的输出和感知器输出的最大区别在于，它的输出不再是0或者1，而是介于0到1之间的一个值。比如0.173...或者0.689等等。这个特性很有用，比如，我们将一幅图像的所有像素点作为神经网络的输入，然后，用神经网络的输出来表示这幅图像素点的平均强度，输出一个0到1之间的值就有了很好的物理解释（如果是感知器的话，只会输出0或者1，这样就不知道强度究竟是多少了）。不过有时候输出一个0到1之间的值也会存在问题，比如我要通过一幅图的所有像素点来判断这幅图是不是数字“9”，显然我们希望神经网络的输出是0还是1，来表示yes or no，也就是感知器那样的二值输出，但是对于sigmoid构成的神经网络而言，其输出为0到1之间的一个值，那么到底是数字“9”还是不是的呢？在实际中，我们通常将感知器的输出小于0.5的时候解释为0，而大于0.5的时候解释为1，也即是说如果sigmoid构成的神经网络的输出如果是大于0.5的值那么久意味着该幅图像表示数字“9”如果输出小于0.5那么就认为该幅图像的输出不是“9”。

##练习

- sigmoid神经元模拟感知器，（part1）
  
  证明：将一个由感知器构成的神经网络中所有权值和偏差都乘以一个正数c后，该网络的特性不变。（c>0）

- sigmoid神经元模拟感知器，（part1）
  和前面的问题一样，假设我们的神经网络还是由感知器构成。选定前面所有感知器的输入，对于感知器中的任何一个输入x都满足$w \cdot x + b \neq 0$ 。然后将所有的感知器换成sigmoid神经元，然后将所有的权重和偏差都乘以一个常数c（c>0）。证明：当$c \rightarrow \infty$时，sigmoid神经元构成的神经网络和感知器构成的神经网络是一样的。如果$w \cdot x + b = 0$时，为什么就不成立了呢？

##神经网络的结构

接下来这部分将介绍神经网络用于手写字识别的内容。在此之前，介绍下神经网络的各个部分结构。

![](http://neuralnetworksanddeeplearning.com/images/tikz10.png)

前面提到过，最左边的称为输入层，其中的神经元称为输入神经元。最右边的称为输出层，其中的神经元叫做输出神经元。中间部分称为隐藏层。上图中只有一层隐藏层，有些神经网络可能有好几层隐藏层，如下图所示：

![](http://neuralnetworksanddeeplearning.com/images/tikz11.png)

输入层和输出层节点的数目通常很容易确定。比如，我们要识别一个手写图像是不是数字9,那么很自然的，假设输入图像是64×64大小的灰度图像，那么我们就有$4,096 = 64 \times 64$ 个输入神经元，对应的输出值位于0到1之间，那么假设输出小于0.5，就判定这幅图像不是数字9，否则的话就是数字9.

不过，中间隐藏层的结构可就没这么容易确定了。这是个经验活。人们在这个过程中总结了许多经验。，后面我们会介绍一些这部分内容。

到目前为止，我们讨论的神经网络都是将某一层神经元的输出传入到下一层的输入，这样的网络通常称为``前向网络``，也就是说，整个网络是单向的，没有形成环。如果网络里有回路（环）的话， $sigma$函数将即受到输出又受到输入的影响。整个网络分析起来会异常复杂，我们将不会涉及该部分。

不过在神经网络领域中，确实有些模型会用到反馈的回路，这类模型通常称为[复现神经网络](http://en.wikipedia.org/wiki/Recurrent_neural_network).其思想是，在神经网络中激活某些神经元一段时间，然后该神经元会激活其它神经元，一段时间后会得到一批激活了的神经网络。之所以这样做不会有问题是因为，神经元之间的相互激活是有一定的延时而不是立即反馈的。

复现神经网络的应用不如反馈神经网络，部分原因是其学习算法不如后者。但是，复现神经网络却更接近我们大脑的工作方式。对某些问题，采用复现神经网络实现起来要比前向反馈型神经网络要容易得多。不过本书只关注反馈型神经外网络。

##一个用于手写数字识别的简单神经网络

介绍完了神经网络，让我们回到手写字识别的问题上来。问题可以分为两部分，首先，将包含手写数字的图像分割称为单独的图像，每个图像包含一个单独的数字。例如，我们可以将下图的数字分割为6副图。

![](http://neuralnetworksanddeeplearning.com/images/digits.png)

![](http://neuralnetworksanddeeplearning.com/images/digits_separate.png)

由人来分割该部分很容易，但是对于电脑来说却没这么轻松。分割好后，程序需要识别每个单独的数字。例如上图中的数字5.

![](http://neuralnetworksanddeeplearning.com/images/mnist_first_digit.png)

接下来我们主要关注第二个问题，也就是分类的问题。第一个问题稍微容易解决些，有很多方法，比如对图像尝试不同的分割方法，使用一个数字分类器来评判分割的结果，如果某个分割的结果比较准确就能得到较高的分数，否则得到较低的打分。其内在的思想是，如果这个分类器在分割图像的时候出现分错的情况，那么很可能在分割其他图像的时候也会分错。这种类似的方法可以很好的解决手写数字的分割问题。因此，后面我们将关注更加有意思而且稍微复杂点的问题，也就是手写数字的识别问题。

我们采用一个三层神经网络来识别手写数字：

![](http://neuralnetworksanddeeplearning.com/images/tikz12.png)

输入层包含包含编码后的输入像素值。在接下来的部分，我们用到的训练数据是28×28的手写数字扫描图像。因此，输入层包含$784 = 28 \times 28 $个神经元。为了简洁起见上图省略了部分神经元。输入的像素值是介于0.0到1.0之间的灰度值。0表示白色，1表示黑色。

第二层是隐藏层，隐藏层的个数先假定为n，我们将尝试使用不同的n值。上图中给出的隐藏层包含15个神经元。

输出层包含10个神经元，如果第一个神经元被激活，也就是说它的输出约等于1，那么就意味着该副手写数字图的内容就是数字0，如果是第二个神经元被激活，那么就意味着该数字应该是1，以此类推。

你可能好奇为什么我们用10个神经元来表示。其实直观的原因是因为我们有0~9一共十个可能的数字。另外一种很自然的设置是，只采用4个神经元作为输出，通过二进制来表示，那么4个输出神经元就能编码所有的10个可能数字，因为$2^4 = 16$比10大。那我们为什么还要用10个神经元的结构呢？那样不会效率很低么？其实这其中的原因完全是经验性的。对于这个特定的问题来讲，10个输出神经元的结构要比4个神经元的输出结构能得到更好的效果。但是，为什么10个神经元的结构就要比4个神经元的结构更好呢？

为了理解这一点，需要从根本上理解神经网络的原理。考虑第一种情况，我们有10个输出神经元，重点看第一个输出神经元，也就是判断输出是否为0的那个。假设隐藏层的第一个神经元用来判断一幅图像是不是像下面这样：

![](http://neuralnetworksanddeeplearning.com/images/mnist_top_left_feature.png)

这个判断的过程很容易实现，可以加大黑色部分像素的权值，减小其他像素的权值，然后他的输出就可以看作是一副原始图像与上图的接近程度。类似的，第二、第三、第四个隐含层的神经元可能是识别以下这几幅图：

![](http://neuralnetworksanddeeplearning.com/images/mnist_other_features.png)

你可能已经猜到了这四幅图合起来一起构成了数字0，也就是下图：

![](http://neuralnetworksanddeeplearning.com/images/mnist_complete_zero.png)

因此，如果这四个隐藏层的神经元被激活了的话，那么我们就能得出结论该图表示数字0，当然，数字0还可能由其它形式的子图构成，但是在这里，我么有信心通过这四个子图得出结论这幅图就是数字0.

假设神经网络就是这么工作的，那么我们就能对前面为什么采用10个输出神经元而不是4个神经元的问题，给出一个近似合理的解释。假设我们的神经网络是有4个输出神经元的结构，那么第一个输出神经元就代表着一个数字的最高位，但是，我们很难讲数字的最高位与数字的形状联系起来。也就是说，一个数字的形状与它是否是最高位的数字并没有关系。

目前为止，这些都只是我们对神经网络的一些探索，并没有谁说神经网络就是这么工作的。也许有那么一种算法可以找到一种赋权值的方法是我们只需要使用4个输出的神经元。但是作为一种启发式思考的方式，我将整个神经网络已经描述的很清楚了，这会节省你构造神经网络结构的不少时间。

##练习

通过在原来的三层网络结构上增加一成额外的输出，我们可以将得到前面提到的二进制表示的结构。多出来的这层是以原来的输出作为输入，如下图所示。找到这样的一组权值来实现这个网络。（假设之前的输出神经元被激活时的输出至少是0.99，没被激活的神经元的输出小于0.01）

##梯度下降

现在我们已经设计好了神经网络，那么怎么用来学习识别数字呢？
首先我们需要数据集来训练。我们使用[MNIST的数据集](http://yann.lecun.com/exdb/mnist/)，其中包含数万张扫描的手写数字图像，以及他们的正确数值。MNIST这个名字是源自[NIST](http://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology) 一个修改后的子集。以下是一个示例：

![](http://neuralnetworksanddeeplearning.com/images/digits_separate.png)

可以看出，这幅图和这章开始时候给出的图是一样的，为了测试我们的神经网络，需要让它去识别自己没见过的数据集。

MNIST数据包含两个部分，一部分包含60000幅图用于训练，这些图是从250人的手写数字中扫描得到的，其中一半是来自美国人口普查部门的雇员，一半来自高中生。这些图像是28乘28的灰度图像。另一部分包含10000幅用于测试，同样也是28乘28的图像。为了体现测试的性能，这部分数据是来自不同于训练集的250个人（也是由人口普查局和高中生组成）。这有助于我们的系统识别来自训练集以外的人所写的数字。

我们用x来表示输入，为了方便，将输入x表示成 $28 \times 28 = 784$维的向量。向量中的每一维表示图像中一个像素点的灰度值。对应的输出设为$y = y(x)$，其中y是一个10维的向量，例如，对于一幅图，x，是数字6，那么$y(x) = (0,0,0,0,0,1,0,0,0)^T$就是神经网络想要得到的输出。其中 T 是 转置操作。将行向量转成列向量。

我们希望找到一种算法，计算出相应的权值后使得神经网络的输出接近我们理想的输出$y(x)$，为了描述这种接近程度，需要用代价函数来描述：

<p>
\begin{eqnarray}  C(w,b) \equiv
  \frac{1}{2} \sum_x \| y(x) - a\|^2.
\tag{6}\end{eqnarray}
</p>

其中，w表示神经网络中的所有权重，b表示所有的偏差，a表示给定x后的实际的输出向量，求和符号表示对所有的训练样本计算差方和。当然输出a依赖于变量x，w，b。但为了简洁起见，这里没有在公式里将这之间的关系表示出来。这里$||v||$就是指向量的长度。C在这里表示二次代价函数，当然，有时候C也用来表示均方差MSE（Mean Squared Error）。公式中$C(w,b)$是非负的。当代价函数很小的时候，也即是$C(w,b) \approx 0$也就意味着对于所有的训练样本$y(x)$接近于实际才输出a，因此，如果算法足够好，可以找到合适的权值和偏差使得代价函数的值$C(w,b) \approx 0$，相反，如果$C(w,b)$的值很大的话，说明y(x)与输出很不一致，因此，训练的目标就成为了找到合适的权值和偏差使得代价函数的输出最小化。接下来，我们将使用梯度下降的算法来实现。

为什么要引入二次代价函数呢？毕竟，我们关注的是图像被正确分类的总数啊？为什么不直接最大化正确分类的数量呢？问题在于，正确分类的个数的函数不是一个连续变化的函数，由于个数是个离散值，这会导致神经网络中的权值和偏差的微小变化会并不会导致正确分类数量出现变化。从而使得权重和偏差的选择上很困难，使用二次代价函数可以看出权值和偏差微小变化时候对二次代价函数输出的影响，从而进一步改进权值和偏差。这就是为什么使用二次代价函数然后再计算准确率。

就算我们需要一个平滑的代价函数，那么为什么就偏偏是前面的二次函数的形式呢？这是否是一个特殊情况？加入换一个代价函数，之后是否会得到完全不一样的权值和偏差了？这些问题值得进一步探讨，后面我们会回顾代价函数，并对其做一些改进，不过，二次代价函数足够解决神经网络中的一些基本问题了，所以我们暂且先来使用它。

总结一下，我们的目标是训练神经网络，然后找到一组权值与偏差来最小化代价函数$C(w,b)$。目前为止，关于怎么解决这个问题还没有一个完整的思路，我们知道神经网络包含权值、偏差，神经元内部通过$\sigma$函数激活，同时我们选定了神经网络的结构，使用的数据集等等。现在，先抛开这些细节，让我们关注在最小化的部分。假设我们已经有一个由许多变量组成的函数，我们想要将它最小化。在这里引入一种叫做梯度下降的算法来解决最小化问题。然后再回过头来考虑特定的神经网络中的最小化问题。

假设我们要最小化函数 $C(v)$ 。其中v可以使任意的变量，$v = v_1,v_2, \ldots$为了方便用图像来表示，我们这里只考虑2维变量：

![](http://neuralnetworksanddeeplearning.com/images/valley.png)

我们希望找到C在哪里取得全局最小值。根据上图，我们一眼就可以看到全局最小值在哪（我给的这个函数太简单了，呵呵）。不过对于一般的函数来说，要复杂得多，可能由许多变量构成。而且几乎不可能通过肉眼就观察到最小值。

解决这个问题的一个办法是，一步一步去找最小值。首先计算偏导，然后用偏导去找C的极值。但当变量很多的时候，这是个浩大的工程量。偏偏神经网络又需要大量的输入变量――最大的神经网络的代价函数可能由数十亿的权值和偏差构成。因此这种方法是行不通的。

计算的方法行不通，不过另外还有一种方法来解决这个问题。试着这么去想，代价函数就像是山谷。我们虚拟出一个球，沿着山谷的坡往下滚。生活经验告诉我们，最后这个球一定会滚到山谷的最低点。那我们可不可以也借助这种思想来求解函数的最小值呢？随机选取一个初始点，然后模拟球的运动方式，最后到达“山谷”的最低点。这个运动过程可以通过计算偏导（或者二阶导）来实现。偏导可以告诉局部最小值在那个方向，从而指导球往那个方向运动。

根据我前面写的这些，你可能以为接下来，我们会写出牛顿方程计算小球的运动轨迹，同时考虑摩擦力和重力。不过实际上没必要考虑太细节的东西。我们是要从这个过程中设计出一种算法来求解最小值。所以，跳出来思考下，假设由你来设计物理定律决定球怎么滚，使得球最后总是到达山谷的最低点呢？

为了让这个问题更清楚点，假设我们给小球在$v_1$方向给一个速度分量$\Delta v_1$，在$v_2$方向给一个速度分量$\Delta v_2$，可以计算出C的变化如下：

<p>
\begin{eqnarray} 
  \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +
  \frac{\partial C}{\partial v_2} \Delta v_2.
\tag{7}\end{eqnarray}
</p>

这里需要找到$\Delta v_1$ 和 $\Delta v_2$ 使得$\Delta C$为负，也就意味着是小球向山谷的最低点方向运动。为了方便，我们用$\Delta v$ 来表示v的变化量，$\Delta v \equiv (Delta v_1,\Delta v_2)^T$, 其中T表示转置操作，将行向量转为列向量。同时，我们将$C$的梯度表示为偏导的形式$(\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2})^T$, 将梯度表示为 \$\nabla C$,如下：

<p>
\begin{eqnarray} 
  \nabla C \equiv \left( \frac{\partial C}{\partial v_1}, 
  \frac{\partial C}{\partial v_2} \right)^T.
\tag{8}\end{eqnarray}
</p>

（此处省略梯度符号$\nabla$的介绍。。。）

简而言之，清明的那个方程可以写为以下形式了：

<p>
\begin{eqnarray} 
  \Delta C \approx \nabla C \cdot \Delta v.
\tag{9}\end{eqnarray}
</p>

这个方程解释了为什么$\nabla C$称为梯度向量，$\nabla C$将v的变化与C联系起来这正是梯度的中作用。更重要的是，可以通过选取$\Delta v$是的$\Delta C$变为负值。
如下式所示：

<p>
\begin{eqnarray} 
  \Delta v = -\eta \nabla C,
\tag{10}\end{eqnarray}
</p>

其中$\eta$非常小的正值（称为学习速率）。从方程9可知：$\Delta C \approx
-\eta \nabla C \cdot \nabla C = -\eta \|\nabla C\|^2$ 当我们根据方程10那样改变v的时候，由于$\| \nabla C \|^2 \geq 0$从而保证：$\Delta C \leq 0$这样C便会一直降低。这正是我们所想要的！在这里我们方程10称作前面梯度下降算法中的那个*球*的"运动法则"。也就是说，我们根据方程10来计算$\Delta v$，然后将球的所在位置$v$移动$\Delta v$。

<p>
\begin{eqnarray}
  v \rightarrow v` = v -\eta \nabla C.
\tag{11}\end{eqnarray}
</p>

然后，反复使用这个法则，从而使$C$一直下降，直到达到我们想要的全局最小。

总结一下，梯度下降算法就是反复计算梯度$\nabla C$，然后往相反的方向运动，沿着山谷的坡度往下"滚"。可视化的表示方法如下：

![](http://neuralnetworksanddeeplearning.com/images/valley_with_ball.png)

需要注意的一点是，这个法则并不会得到和实际的物理运动相同的结果，因为，在实际生活中，小球是有惯性的，当球到达最低点后会越过最低点继续往上爬。直到因为受摩擦的影响最后回落在最低点。相比之下，根据我们的法则，我们所选择的$\Delta v$会一直让小球“立即往下滚”。这样显然更有利于找到最小值。

为了保证梯度下降算法正确的工作，需要选取一个足够小的$\eta$值以保证方程9中约等于条件的成立。否则的话，最后可能会得到$\Delta C >0。同时，$\eta$也不能太小了，因为这会使得$\Delta v$太小，从而梯度下降算法会很慢。实际使用中，通常会根据方程9调整到一个合适的大小，同时算法不至于收敛得太慢。后面会讲到如何做到这点。

前面解释梯度下降的时候，C只是两个变量的函数。实际上，当C是许多个变量的函数时，梯度下降算法也能很好的工作。假设C是m个变量，$v_1, v_2, ..., v_m$构成的函数，C的变化量$\Delta C$可以由小的变化量$\Delta v = (\Delta v_1, ..., \Delta v_m)^T$产生：

<p>
\begin{eqnarray} 
  \Delta C \approx \nabla C \cdot \Delta v,
\tag{12}\end{eqnarray}
</p>

其中，梯度$\nabla C$ 是一个向量

<p>
\begin{eqnarray}
  \nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \ldots, 
  \frac{\partial C}{\partial v_m}\right)^T.
\tag{13}\end{eqnarray}
</p>

和两个变量时候的情况一样，

<p>
\begin{eqnarray}
  \Delta v = -\eta \nabla C,
\tag{14}\end{eqnarray}
</p>

保证方程12的得到的$\Delta C$是负的。这样即使C是多个变量所构成的函数，通过反复使用下面的更新规则，一样可以达到最小值。

<p>
\begin{eqnarray}
  v \rightarrow v` = v-\eta \nabla C.
\tag{15}\end{eqnarray}
</p>

你认为这条更新规则就定义为梯度下降算法。这个规则通过发反复改变位置vl唉找到函数C的最小值。不过这个规则并非总是有效的，有一些因素会让梯度下降算法找不到全局最小值，后面的章节会讨论这个问题。但是在实际中，梯度下降算法效果相当好，在神经网络算法中，我们会看到它在最小化代价函数的时候相当有效。

实际上，有人认为梯度下降算法是最佳的搜索最小值的策略。假设，我们希望移动$\Delta v$之后，C尽可能的变小。也就是最小化$\Delta C approx \nabla C ・ \Delta v $.同时，限制步长$||\Delta v|| = \epsilon$（$\epsilon$ 是某一大于0的值）。通俗的说，就是要找到某个方向，使得移动一步固定的大小后，C尽可能的降低。可以证明，使得$\nabla C ・ \Delta v$最小化的$\Delta v = - \eta \nabla C$， 其中，$\eta = \epsilon / ||\nabla C||$是由步长$\Delta v|| = \epsilon$决定。因此，梯度下降算法所找到的方向，在移动一步后，能够将C尽可能减小。

##练习

    证明上一段中的推论。提示：如果你对[柯西不等式](http://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality)不太熟的话，提前熟悉下，会对你有帮助的。

    前面已经解释了C是两个变量的函数以及大于两个变量时的情况。那么，假设C是一个变量的函数时的情况呢？顺便给出几何意义上的解释。

人们研究过许多种梯度算法的变形，包括一些模拟真实物理中球的运动的变形。这类模拟球的变形算法有一些优势，但是缺陷也很明显：通常需要计算C的二阶导，计算量较大。为什么计算量比较大？假设我们希望计算出二阶偏导$\partial^2 C/ \partial v_j \partial v_k$。如果有上百万个变量$\v_j$，那么我们需要计算1兆个二阶偏导！尽管有一些技巧可以避免这个问题，并且目前这也是一个比较热的领域，但是本书将主要采用梯度下降（及某些变种）来实现神经网络。

那么怎么将梯度下降应用到神经网络中呢？主要就是用梯度下降来找到权值$w_k$和偏差$b_l$来最小化方程6中的代价函数。为了说明怎么工作的，我们重新定义梯度下降的更新规则，用权重和偏差代替原来的变量$v_j$，也就是说，现在的“位置”是由两部分构成，$w_k 和b_l$,梯度向量$\nabla C$对应的部分是 $\partial C / \partial w_k$ 和 $\partial C / \partial b_l$，重写更新规则后，得到：

<p>
\begin{eqnarray}
  w_k & \rightarrow & w_k' = w_k-\eta \frac{\partial C}{\partial w_k} \tag{16}\\
  b_l & \rightarrow & b_l' = b_l-\eta \frac{\partial C}{\partial b_l}.
\tag{17}\end{eqnarray}
</p>

反复迭代这条规则后，就能不断降低C，从而有可能找到代价函数的最小值。换言之，这个规则可以用于神经网络的学习。

在这里应用梯度下降做出了一些改变，我们在后面几章会深入学习。但是现在我想着重强调一点。让我们先回到前面的二次代价函数（方程6），注意该代价函数的形式是$C = \frac{1}{n} \sum_x C_x$，也就是说，C是每个独立训练样本的损失值的平均$C_x \equiv \frac{\|y(x)-a\|^2}{2}$。实际中，为了计算梯度$\nabla C$，需要对于每个独立的训练样本计算梯度$\nabla C_x$，然后再求平均，即$\nabla C = \frac{1}{n}
\sum_x \nabla C_x$,然而，当样本数量非常大的时候，这需要相当多的时间，学习的速度将非常慢。

一种称为**随机梯度下降*的算法能够用于加速学习过程。其思想是通过计算训练集中的一部分随机样本的$\nabla C_x$来估计梯度$\nabla C$。计算这些小样本上的平均梯度可以很快的得到真实梯度的估计$\nabla C$，从而加速梯度下降也就是学习的过程。

随机梯度下降算法从训练集中随机选取少量样本，将其标注为$X_1, X_2, ..., X_m$，称作*mini-batch*。假设m足够大的话，$\nabla C_{X_j}$的平均值将接近所有$\nabla C_x$的平均值。也就是说：

<p>
\begin{eqnarray}
  \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C,
\tag{18}\end{eqnarray}
</p>

其中第二个求和符号是针对所有的样本。反转等式的两边，我们得到：

<p>
\begin{eqnarray}
  \nabla C \approx \frac{1}{m} \sum_{j=1}^m \nabla C_{X_{j}},
\tag{19}\end{eqnarray}
</p>

也就是说，可以通过计算mini-batch的梯度来估计全局的梯度。
将这部分内容与神经网络联系起来，假设$w_k 和 b_l$分别表示神经网络中的权重和偏差，那么，随机梯度下降算法更新权重的公式如下：

<p>
\begin{eqnarray} 
  w_k & \rightarrow & w_k' = w_k-\frac{\eta}{m}
  \sum_j \frac{\partial C_{X_j}}{\partial w_k} \tag{20}\\
  
  b_l & \rightarrow & b_l' = b_l-\frac{\eta}{m}
  \sum_j \frac{\partial C_{X_j}}{\partial b_l},
\tag{21}\end{eqnarray}
</p>

其中，求和符号表示对当前mini-batch中的所有样本$X_j$的偏导求和。然后再随机选取一个ini-batch并训练，直到遍历了一遍所有的训练集。我们把这个过程称为**一轮**，然后，我们再开始新的一轮。

附带多说几句，采用mini-batch的方式后，代价函数和权重以及偏差并没有变化。在方程6中，我们将其乘以了$\frac{1}{n}$。不过人们一般倾向于去掉$\frac{1}{n}$，只对每个样本的损失求和而不是求平均。这在样本数量未知的时候会比较有用，比如，在训练集数据是实时产生的情况下。因此，类似的，mini-batch的更新公式20和21有时候也会将$\frac{1}{m}$的部分移去，从概念上来说，这会造成一点点的不同，因为这相当于重新调整了学习速率$\eta$,但是在比较实现细节不同之处的时候，这点需要留意。

随机梯度下降就像是民意测验民意调查，抽样出一小部分样本来计算显然要比计算全局的梯度要容易得多，就好像民意调查要比全民选举要容易得多。例如，在MNIST数据集中，训练集的样本大小是60000，假设选取一个mini-batch的大小为10，这意味着其速度相比全局的方法提升了6000倍！显然，这样的估计不是完美的――会出现统计上的波动――但是它并不需要完美，我们只关心每一次更新后，都会让C减小。也就是说，我们并不需要一个完全精确的梯度。实际中，随机梯度下降算法在神经网络中有着广泛的应用，并且这将是本书最基础的学习算法。

##练习

    随机梯度下降算法的一个极限情况是，让mini-batch的大小是1，也就是说，每给一个训练样本就根据以下规则更新一次权重和偏差：
<p>
$w_k \rightarrow w_k' =  w_k - \eta \partial C_x / \partial w_k $ 和$b_l \rightarrow b_l' =  b_l - \eta \partial C_x / \partial b_l$
</p>

这种学习策略叫做在线学习又或者是增量学习，在线学习过程中，神经网络每次学习一个样本（就和人类一样），相对于采用mini-batch的大小为20的算法，请指出该方法的一个优势和一个劣势。

最后让我们一起讨论一个刚接触梯度下降算法的人会感到困惑的地方，以此来结束本部分的内容。在神经网络中，代价函数C,是一个由多变量（权值和偏差）构成的函数，并在一个高位空间中定义了一个表面。有人可能会想，“我想将其他维都可视化的表示出来，可是我没法思考4维空间，更别说5维甚至500万维的了”，那到底是这些人缺少某些特殊的能力呢，还是数学家们有某些超能力可以看到高位空间？显然不是。就算是最厉害的数学家们也很难清楚地描述高维空间。他们所使用的技巧是采用一些其他方式来表示高位空间到底是怎么回事。和我们前面所用到的方法一样，我们采用一种代数上的表达方式（而不是视觉上的）来表示$\Delta C$，从而描述如何运动使得C减小。那些善于在高位空间思考的人有着一堆类似的方法，我们这里用到的表示方法也只是其中的一个例子。这些技巧也许和可视化描述三维空间的方法相比起来并不那么简单，但是一旦你建立了一系列这类技巧，你能够很轻松地在高位空间思考。在这里我并不打算深入进去，不过如果你感兴趣的话，可以看看这些[讨论](http://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking)，这里讨论的一些技巧可能相对来说比较复杂，不过大部分都是很直观且易于接受的方法，能被所有人掌握。

##动手实现一个能识别数字的神经网络

现在，让我们一起动手用随机梯度下降算法写一个能够识别手写数字的程序。首先，我们需要获取MNIST的数据，如果你会用git的话，可以将本书的代码拷一份。

    :::
    git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git

如果你不使用git的话，可以在[这里](https://github.com/mnielsen/neural-networks-and-deep-learning/archive/master.zip)下载

一开始谈到MNIST数据的时候，我说过它分为60000个训练集图像和10000个测试图像，这是MNIST的官方说法。不过呢，在这里我们把数据稍微切分一下，测试集保持原样，，将训练集的60000幅图分为两部分，50000用于训练，10000用于验证。在本章将不会使用到验证集，不过后面会用到这部分将用于设置神经网络的参数（比如学习速率等等）。尽管验证集并不再原始的MNIST数据中有提到。不过大多数使用MNIST数据集的人更倾向于采用这种做法。以下当我提到MNIST训练集的时候，均是指这50000幅图，而不是原始的60000幅图。

除了MNIST数据之外，还需要一个python的库[Numpy](http://numpy.org/)，用于线性计算。如果你还没有安装，可以从[这里](http://www.scipy.org/install.html)获取到。

在给出完整代码之前，首先来介绍下神经网络代码中的核心部分。最重要的是Network的类，用于描述神经网络。以下是用来初始化Network对象的代码：

    :::python
    class Network():

    def __init__(self, sizes):
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x) 
                        for x, y in zip(sizes[:-1], sizes[1:])]

这段代码中``sizes``表示神经网络每层的神经元个数，例如，想要构建这样一个神经网络，第一层有两个神经元，第二层有3个，最后一层有1个，那么可以这么初始化：

    :::python
    net = Network([2, 3, 1])

``Network``对象中的权值和偏差都是随机初始化的，用的是Numpy 的``np.random.randn``函数产生的均值为0,标准差为1的高斯分布。该初始值便是随机梯度下降的起点。后面的章节将会讲到如何找到更好的权值与偏差的初始值。在``Network``初始化的过程中，我们假设第一层是输入层，并忽略了这些神经元的偏差部分，因为偏差只在计算后面层的输出时候有用到。

另外，偏差和权重都以Numpy中的矩阵形式保存着，例如，``net.weights[1]``保存着连接第二层和第三层神经元之间的权重（注意不是第一层和第二层之间的权值，因为python中的序号从0开始）。``net.weights[1]``的表达太繁琐了点，用矩阵w来代替下，$w_jk$表示连接第二层第k个神经元和第三层第j个神经元之间的权重。看起来，w的下标j和k的顺序是不是标反了？这样子标的好处是，计算第三层的输出时，可以通过以下式子得到：

<p>
\begin{eqnarray} 
  a' = \sigma(w a + b).
\tag{22}\end{eqnarray}
</p>

将这个式子拆开来一点点分析，a是第二层的输出，为了得到a',将a乘以权重矩阵w，然后加上偏差向量b，并对向量wa+b中的每个元素经函数$\sigma$变换后即可。（注：这里函数$\sigma$实际上是向量化的函数，具体可以阅读numpy中相关部分）很容易证明，方程22得到的结果合前面方程4得到的结果是一样的，只不过是采用了矩阵的表达方式。

##练习

    写出方程22拆开后的形式，并证明其结果和方程4的结果是一样的。

理解前面这些后，很容易就可以从一个``Network``的实例对象中计算输出了。首先定义一个sigmoid函数，然后用numpy的向量化函数将其表示为向量函数的形式：

    :::python
    def sigmoid(z):
    return 1.0/(1.0+np.exp(-z))

    sigmoid_vec = np.vectorize(sigmoid)

然后，给``Network``类添加``feedforward``方法，接受一个输入a作为参数，计算其输出。该方法所做的事情就是对每一层计算方程22：

    def feedforward(self, a):
        """Return the output of the network if "a" is input."""
        for b, w in zip(self.biases, self.weights):
            a = sigmoid_vec(np.dot(w, a)+b)
        return a

``Network``最主要的任务是*学习*，这里，给出SGD方法用于实现随机梯度下降算法。这段代码看起来稍微有点复杂，我将它分解后一点点解释。

    def SGD(self, training_data, epochs, mini_batch_size, eta,
            test_data=None):
        """Train the neural network using mini-batch stochastic
        gradient descent.  The "training_data" is a list of tuples
        "(x, y)" representing the training inputs and the desired
        outputs.  The other non-optional parameters are
        self-explanatory.  If "test_data" is provided then the
        network will be evaluated against the test data after each
        epoch, and partial progress printed out.  This is useful for
        tracking progress, but slows things down substantially."""
        if test_data: n_test = len(test_data)
        n = len(training_data)
        for j in xrange(epochs):
            random.shuffle(training_data)
            mini_batches = [
                training_data[k:k+mini_batch_size]
                for k in xrange(0, n, mini_batch_size)]
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta)
            if test_data:
                print "Epoch {0}: {1} / {2}".format(
                    j, self.evaluate(test_data), n_test)
            else:
                print "Epoch {0} complete".format(j)

``training_data``是由``(x,y)``构成的列表，表示训练集中的输入以及对应的输出。``epochs`` 和 ``mini_batch_size``分别表示对训练集遍历的次数（也即前面提到的**轮数**）和每个mini_batch的大小。``eta``表示学习速率$\eta$，如果参数``test_data``提供了的话，程序会在每遍历完一轮后计算在测试集上的误差并报告精度和结果。

在下面的代码中，每一轮开始先将训练集随机打乱，并分为一个个mini-batches，这是最简单的不放回抽样，对于每个mini-batch进行一次梯度下降计算，这部分由代码``self.update_mini_batch(mini_batch, eta)``实现，该部分根据mini_batch中的 每个样本计算梯度后更新神经网络的权值和偏差。一下是更新权值和偏差的代码：

    def update_mini_batch(self, mini_batch, eta):
        """Update the network's weights and biases by applying
        gradient descent using backpropagation to a single mini batch.
        The "mini_batch" is a list of tuples "(x, y)", and "eta"
        is the learning rate."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
             nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
             nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw 
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb 
                       for b, nb in zip(self.biases, nabla_b)]

其中，大部分工作都是由下面这行代码完成：

    delta_nabla_b, delta_nabla_w = self.backprop(x, y)

这部分包含了反向传播算法，用来快速计算代价函数的梯度。也就是说，更新函数``update_mini_batch``仅仅是计算每个mini_batch中的样本的梯度后，更新整个网络的权值``self.weights``和偏差``self.biases``。

暂时先把``self.bakprop``部分的代码放一边，在下一章将会重点讲反馈是如何工作的。现在，你就把它想象成一个黑箱，给定一个样本，能够计算出其梯度。

来回顾一下整个程序。除了``self.backprop``部分之外，其他部分都很容易理解，最重要的部分是``self.SGD``和``slef.update_mini_batch``，另外，``sigmoid_prime``用于计算$\sigma$函数的导数，以及其向量化的表示``sigmoid_prime_vec``,至于``self.cost_derivative``这个函数待会再说。基本上结合代码中的注释和我前面的解释，你能够掌握整个代码的核心部分了。下一节会更深入的讲解。虽然整段代码看起来稍微有点长，不过，其实大部分都是注释以方便理解，去掉注释和空白行后，整个代码只有74行！所有的代码都可以在GitHub上[找到](https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/code/network.py)：

"""
network.py
~~~~~~~~~~

A module to implement the stochastic gradient descent learning
algorithm for a feedforward neural network.  Gradients are calculated
using backpropagation.  Note that I have focused on making the code
simple, easily readable, and easily modifiable.  It is not optimized,
and omits many desirable features.
"""

#### Libraries
# Standard library
import random

# Third-party libraries
import numpy as np

class Network():

    def __init__(self, sizes):
        """The list ``sizes`` contains the number of neurons in the
        respective layers of the network.  For example, if the list
        was [2, 3, 1] then it would be a three-layer network, with the
        first layer containing 2 neurons, the second layer 3 neurons,
        and the third layer 1 neuron.  The biases and weights for the
        network are initialized randomly, using a Gaussian
        distribution with mean 0, and variance 1.  Note that the first
        layer is assumed to be an input layer, and by convention we
        won't set any biases for those neurons, since biases are only
        ever used in computing the outputs from later layers."""
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x) 
                        for x, y in zip(sizes[:-1], sizes[1:])]

    def feedforward(self, a):
        """Return the output of the network if ``a`` is input."""
        for b, w in zip(self.biases, self.weights):
            a = sigmoid_vec(np.dot(w, a)+b)
        return a

    def SGD(self, training_data, epochs, mini_batch_size, eta,
            test_data=None):
        """Train the neural network using mini-batch stochastic
        gradient descent.  The ``training_data`` is a list of tuples
        ``(x, y)`` representing the training inputs and the desired
        outputs.  The other non-optional parameters are
        self-explanatory.  If ``test_data`` is provided then the
        network will be evaluated against the test data after each
        epoch, and partial progress printed out.  This is useful for
        tracking progress, but slows things down substantially."""
        if test_data: n_test = len(test_data)
        n = len(training_data)
        for j in xrange(epochs):
            random.shuffle(training_data)
            mini_batches = [
                training_data[k:k+mini_batch_size]
                for k in xrange(0, n, mini_batch_size)]
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta)
            if test_data:
                print "Epoch {0}: {1} / {2}".format(
                    j, self.evaluate(test_data), n_test)
            else:
                print "Epoch {0} complete".format(j)

    def update_mini_batch(self, mini_batch, eta):
        """Update the network's weights and biases by applying
        gradient descent using backpropagation to a single mini batch.
        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``
        is the learning rate."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw 
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb 
                       for b, nb in zip(self.biases, nabla_b)]

    def backprop(self, x, y):
        """Return a tuple ``(nabla_b, nabla_w)`` representing the
        gradient for the cost function C_x.  ``nabla_b`` and
        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar
        to ``self.biases`` and ``self.weights``."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        # feedforward
        activation = x
        activations = [x] # list to store all the activations, layer by layer
        zs = [] # list to store all the z vectors, layer by layer
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid_vec(z)
            activations.append(activation)
        # backward pass
        delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime_vec(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
        # Note that the variable l in the loop below is used a little
        # differently to the notation in Chapter 2 of the book.  Here,
        # l = 1 means the last layer of neurons, l = 2 is the
        # second-last layer, and so on.  It's a renumbering of the
        # scheme in the book, used here to take advantage of the fact
        # that Python can use negative indices in lists.
        for l in xrange(2, self.num_layers):
            z = zs[-l]
            spv = sigmoid_prime_vec(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * spv
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)

    def evaluate(self, test_data):
        """Return the number of test inputs for which the neural
        network outputs the correct result. Note that the neural
        network's output is assumed to be the index of whichever
        neuron in the final layer has the highest activation."""
        test_results = [(np.argmax(self.feedforward(x)), y) 
                        for (x, y) in test_data]
        return sum(int(x == y) for (x, y) in test_results)
        
    def cost_derivative(self, output_activations, y):
        """Return the vector of partial derivatives \partial C_x /
        \partial a for the output activations."""
        return (output_activations-y) 

#### Miscellaneous functions
def sigmoid(z):
    """The sigmoid function."""
    return 1.0/(1.0+np.exp(-z))

sigmoid_vec = np.vectorize(sigmoid)

def sigmoid_prime(z):
    """Derivative of the sigmoid function."""
    return sigmoid(z)*(1-sigmoid(z))

sigmoid_prime_vec = np.vectorize(sigmoid_prime)

那么这段代码识别手写数字的准确率如何呢？首先载入数据，这里要用到``mnist_loader.py``，然后在python解释器中运行以下脚本：

>>> import mnist_loader
>>> training_data, validation_data, test_data = \
... mnist_loader.load_data_wrapper()

当然，这部分也可以写成一个单独的代码，不过如果你跟着在做的话，建议使用python解释器去执行，更方便点。

首先新建一个中间层有30个神经元的神经网络，如下所示：

    >>> import network
    >>> net = network.Network([784, 30, 10])

然后，用随机梯度下降算法对整个训练集学习30遍，每个mini-batch的大小是10，学习速率是3.0

    >>> net.SGD(training_data, 30, 10, 3.0, test_data=test_data)

如果你跟着一步步在做的话，稍微耐心点，这段代码跑完需要点时间（这要你机器不是很差劲，每一轮也就分分钟的事）。接着往下读。如果你实在很着急想看到结果，嗯，适当降低遍历的轮数或者减少中间层的个数，又或者把训练集弄小点。需要注意的是，实际使用中的代码要比这快得多，这部分代码是为了方便你理解神经网络的工作原理的，并没有追求多么高的效率。一旦训练好了之后，计算起来就会很快的了，比如，我们可以将训练好了的权值和偏差导入到javascript中或者是移动设备上。下面这部分描述了每一轮结束后，神经网络在测试集上的识别准确率。可以看到，第一轮便识别出了10000幅图中的9129幅图。随着轮数的增加，准确率也在增加。

Epoch 0: 9129 / 10000
Epoch 1: 9295 / 10000
Epoch 2: 9348 / 10000
...
Epoch 27: 9528 / 10000
Epoch 28: 9542 / 10000
Epoch 29: 9534 / 10000

也就是说，神经网络的识别率达到了95%，（最高在第28轮达到了95.42%)，作为第一次尝试，这个结果相当不错了！不过要提醒你一下，你在自己机器上的运行结果可能跟这个不一样，这是由于随机初始化权重的影响。

继续上面的实验，这次把中间层的神经元个数改为100，再跑一次，（别傻等着，那得多跑一会的，接着往下看）。

>>> net = network.Network([784, 100, 10])
>>> net.SGD(training_data, 30, 10, 3.0, test_data=test_data)

这一次，准确率提高到了96.59%,至少在这个例子中，中间层的个数越多，结果越好（不过有读者反映，中间层个数变多的时候，反而效果下降了，不过使用第三章中介绍的技巧后，将会避免这种情况）

不过，为了达到这样的准确度，需要仔细筛选训练的轮数、mini-batch的大小以及学习速率等等。正如我前面讲到了的，这些参数称为*超参数*，主要是为了区别于权值和偏差这类参数。如果运气不好选的超参数很糟糕，得到的结果将惨不忍睹。比如下面这组：

>>> net = network.Network([784, 100, 10])
>>> net.SGD(training_data, 30, 10, 0.001, test_data=test_data)

Epoch 0: 1139 / 10000
Epoch 1: 1136 / 10000
Epoch 2: 1135 / 10000
...
Epoch 27: 2101 / 10000
Epoch 28: 2123 / 10000
Epoch 29: 2142 / 10000

尽管如此，你可以看到整个网络的识别效果还是在缓慢提升的。这意味着需要提升学习速率了，比如将学习速率改为0.01.（如果每次改进能够提高准确率，那么久继续改进！）直到最后，学习速率将会稳定在$\eta = 1.0$附近，非常接近我们前面的实验中用到的参数。所以，尽管一开始选了个很糟糕的参数，不顾，通过改进超参数，可以将识别的准确率提升到一个还不错的水平。

通常来说，调试神经网络是个很有挑战的活。尤其是在选定的超参数所参生的结果近似于随机噪声的时候。假设我们使用30个神经元的中间层，但是将学习速率改为$\eta = 100.0$，

>>> net = network.Network([784, 30, 10])
>>> net.SGD(training_data, 30, 10, 100.0, test_data=test_data)

结果如下

Epoch 0: 1009 / 10000
Epoch 1: 1009 / 10000
Epoch 2: 1009 / 10000
Epoch 3: 1009 / 10000
...
Epoch 27: 982 / 10000
Epoch 28: 982 / 10000
Epoch 29: 982 / 10000

假设一开始我们就碰到了这个问题。从前面的实验里，我们当然知道，正确的做法应该是降低学习速率。但是如果**第一次**就碰到这类问题，显然无法知道该怎么调整参数。我们可能担心，这不只是学习速率，还可能是神经网络的其他部分，又或者是因为我们给的训练集不够大，还是因为轮次不够多，又或者是学习速率**太小*了？第一次碰到这类问题的时候，往往会不知所措。

从上面的这些讨论可知，调试神经网络不仅仅是个编程的活，还是个有技巧的艺术活。我们需要一种启发式的方法来选择超参数以及神经网络的结构，这部分内容将贯穿整本书。

##练习

    尝试构建一个只有两层的神经网络，包含输入层（784个神经元）和输出层（10个神经元），没有中间层，并用随机梯度下降法训练这个网络，看看能达到多高的精度。

前面我跳过了如何载入MNIST数据的，为了保持内容的完整性，这里给出其代码。其原始数据结构在注释行中有描述，并保存在Numpy的``ndarray``对象中。

"""
mnist_loader
~~~~~~~~~~~~

A library to load the MNIST image data.  For details of the data
structures that are returned, see the doc strings for ``load_data``
and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the
function usually called by our neural network code.
"""

#### Libraries
# Standard library
import cPickle
import gzip

# Third-party libraries
import numpy as np

def load_data():
    """Return the MNIST data as a tuple containing the training data,
    the validation data, and the test data.

    The ``training_data`` is returned as a tuple with two entries.
    The first entry contains the actual training images.  This is a
    numpy ndarray with 50,000 entries.  Each entry is, in turn, a
    numpy ndarray with 784 values, representing the 28 * 28 = 784
    pixels in a single MNIST image.

    The second entry in the ``training_data`` tuple is a numpy ndarray
    containing 50,000 entries.  Those entries are just the digit
    values (0...9) for the corresponding images contained in the first
    entry of the tuple.

    The ``validation_data`` and ``test_data`` are similar, except
    each contains only 10,000 images.

    This is a nice data format, but for use in neural networks it's
    helpful to modify the format of the ``training_data`` a little.
    That's done in the wrapper function ``load_data_wrapper()``, see
    below.
    """
    f = gzip.open('../data/mnist.pkl.gz', 'rb')
    training_data, validation_data, test_data = cPickle.load(f)
    f.close()
    return (training_data, validation_data, test_data)

def load_data_wrapper():
    """Return a tuple containing ``(training_data, validation_data,
    test_data)``. Based on ``load_data``, but the format is more
    convenient for use in our implementation of neural networks.

    In particular, ``training_data`` is a list containing 50,000
    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray
    containing the input image.  ``y`` is a 10-dimensional
    numpy.ndarray representing the unit vector corresponding to the
    correct digit for ``x``.

    ``validation_data`` and ``test_data`` are lists containing 10,000
    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional
    numpy.ndarry containing the input image, and ``y`` is the
    corresponding classification, i.e., the digit values (integers)
    corresponding to ``x``.

    Obviously, this means we're using slightly different formats for
    the training data and the validation / test data.  These formats
    turn out to be the most convenient for use in our neural network
    code."""
    tr_d, va_d, te_d = load_data()
    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]
    training_results = [vectorized_result(y) for y in tr_d[1]]
    training_data = zip(training_inputs, training_results)
    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]
    validation_data = zip(validation_inputs, va_d[1])
    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]
    test_data = zip(test_inputs, te_d[1])
    return (training_data, validation_data, test_data)

def vectorized_result(j):
    """Return a 10-dimensional unit vector with a 1.0 in the jth
    position and zeroes elsewhere.  This is used to convert a digit
    (0...9) into a corresponding desired output from the neural
    network."""
    e = np.zeros((10, 1))
    e[j] = 1.0
    return e

前面我说这个程序得到了比较好的结果，那么这个“好”是相对什么而言的呢？所以，需要有一个基准（采用除神经网络之外的算法）作为比较。最简单的就是随机猜，其准确率大概10%，神经网络可比这强多了！

那，稍微强一点的基准呢？比如说，计算图像的颜色深浅。例如，数字2的颜色显然要比数字1的颜色深一些，如下图所示：

![](http://neuralnetworksanddeeplearning.com/images/mnist_2_and_1.png)

也就是说，计算每个数字所对应图像的平均灰度值。给定一幅新的图时，计算其平均灰度值，然后看更接近哪个数字的平均灰度值